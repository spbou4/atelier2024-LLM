# Architecture du modèle

La plupart des modèles neuronaux de transduction de séquences compétitifs possèdent une structure encodeur-décodeur [(cite)](https://arxiv.org/abs/1409.0473). Ici, l'encodeur associe une séquence d'entrée de représentations symboliques $(x_1, ..., x_n)$ à une séquence de représentations continues $\mathbf{z} = (z_1, ..., z_n)$. Étant donné $\mathbf{z}$, le décodeur génère ensuite une séquence de sortie $(y_1,...,y_m)$ de symboles, un élément à la fois. À chaque étape, le modèle est auto-régressif [(cite)](https://arxiv.org/abs/1308.0850), utilisant les symboles précédemment générés comme entrée supplémentaire lors de la génération du suivant.

``` python
class EncoderDecoder(nn.Module):
    """
    A standard Encoder-Decoder architecture. Base for this and many
    other models.
    """

    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):
        super(EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.tgt_embed = tgt_embed
        self.generator = generator

    def forward(self, src, tgt, src_mask, tgt_mask):
        "Take in and process masked src and target sequences."
        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)

    def encode(self, src, src_mask):
        return self.encoder(self.src_embed(src), src_mask)

    def decode(self, memory, src_mask, tgt, tgt_mask):
        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)
        
      
class Generator(nn.Module):
    "Define standard linear + softmax generation step."

    def __init__(self, d_model, vocab):
        super(Generator, self).__init__()
        self.proj = nn.Linear(d_model, vocab)

    def forward(self, x):
        return log_softmax(self.proj(x), dim=-1)
```

## Encodeur et Décodeur

### Encodeur

L'encodeur est composé d'une pile de $N=6$ couches identiques.

``` python
def clones(module, N):
    "Produce N identical layers."
    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])
    

class Encoder(nn.Module):
    "Core encoder is a stack of N layers"

    def __init__(self, layer, N):
        super(Encoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, mask):
        "Pass the input (and mask) through each layer in turn."
        for layer in self.layers:
            x = layer(x, mask)
        return self.norm(x)
        
class LayerNorm(nn.Module):
    "Construct a layernorm module (See citation for details)."

    def __init__(self, features, eps=1e-6):
        super(LayerNorm, self).__init__()
        self.a_2 = nn.Parameter(torch.ones(features))
        self.b_2 = nn.Parameter(torch.zeros(features))
        self.eps = eps

    def forward(self, x):
        mean = x.mean(-1, keepdim=True)
        std = x.std(-1, keepdim=True)
        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2
        

class SublayerConnection(nn.Module):
    """
    A residual connection followed by a layer norm.
    Note for code simplicity the norm is first as opposed to last.
    """

    def __init__(self, size, dropout):
        super(SublayerConnection, self).__init__()
        self.norm = LayerNorm(size)
        self.dropout = nn.Dropout(dropout)

    def forward(self, x, sublayer):
        "Apply residual connection to any sublayer with the same size."
        return x + self.dropout(sublayer(self.norm(x)))
        

class EncoderLayer(nn.Module):
    "Encoder is made up of self-attn and feed forward (defined below)"

    def __init__(self, size, self_attn, feed_forward, dropout):
        super(EncoderLayer, self).__init__()
        self.self_attn = self_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 2)
        self.size = size

    def forward(self, x, mask):
        "Follow Figure 1 (left) for connections."
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))
        return self.sublayer[1](x, self.feed_forward)
      
      
```

### Decodeur

Le décodeur est également composé d'une pile de $N=6$ couches identiques.

``` python
class Decoder(nn.Module):
    "Generic N layer decoder with masking."

    def __init__(self, layer, N):
        super(Decoder, self).__init__()
        self.layers = clones(layer, N)
        self.norm = LayerNorm(layer.size)

    def forward(self, x, memory, src_mask, tgt_mask):
        for layer in self.layers:
            x = layer(x, memory, src_mask, tgt_mask)
        return self.norm(x)
        
  
  class DecoderLayer(nn.Module):
    "Decoder is made of self-attn, src-attn, and feed forward (defined below)"

    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):
        super(DecoderLayer, self).__init__()
        self.size = size
        self.self_attn = self_attn
        self.src_attn = src_attn
        self.feed_forward = feed_forward
        self.sublayer = clones(SublayerConnection(size, dropout), 3)

    def forward(self, x, memory, src_mask, tgt_mask):
        "Follow Figure 1 (right) for connections."
        m = memory
        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))
        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))
        return self.sublayer[2](x, self.feed_forward)
        
  def subsequent_mask(size):
    "Mask out subsequent positions."
    attn_shape = (1, size, size)
    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(
        torch.uint8
    )
    return subsequent_mask == 0


def example_mask():
    LS_data = pd.concat(
        [
            pd.DataFrame(
                {
                    "Subsequent Mask": subsequent_mask(20)[0][x, y].flatten(),
                    "Window": y,
                    "Masking": x,
                }
            )
            for y in range(20)
            for x in range(20)
        ]
    )

    return (
        alt.Chart(LS_data)
        .mark_rect()
        .properties(height=250, width=250)
        .encode(
            alt.X("Window:O"),
            alt.Y("Masking:O"),
            alt.Color("Subsequent Mask:Q", scale=alt.Scale(scheme="viridis")),
        )
        .interactive()
    )


show_example(example_mask)
```

# LoRA Finetuning

**Low-Rank Adaptation (LoRA)** est une technique innovante conçue pour le fine-tuning efficace des grands modèles de langage (LLM). Plongeons dans ce qui fait de LoRA un changement de paradigme dans le domaine de l'apprentissage automatique et du traitement du langage naturel :

## Qu'est-ce que LoRA ?

-   **Concept :** LoRA introduit une décomposition de bas rang pour les matrices de poids au sein des modèles de transformateurs.
-   **Efficacité :** En entraînant seulement un petit nombre de paramètres supplémentaires, LoRA réduit considérablement le coût computationnel.

## Avantages de LoRA

-   **Vitesse :** Le fine-tuning avec LoRA est significativement plus rapide en raison du moindre nombre de paramètres mis à jour.
-   **Personnalisation :** Cela permet aux data scientists d'adapter de grands modèles à leurs tâches spécifiques sans un réentraînement extensif.

## Quelques Cas d'Usage

-   **IA Personnalisée :** Personnalisez les modèles d'IA pour comprendre des jargons ou concepts spécifiques dans des domaines de niche.
-   **Performance Optimisée :** Améliorez la performance sur des tâches comme l'analyse de sentiments ou le résumé de documents avec un fine-tuning spécifique au domaine.
-   **Déploiement Efficace :** Capable de déployer un grand modèle de base LLM et plusieurs petits adaptateurs LoRA, au lieu de devoir déployer plusieurs grands modèles.

Dans les sections suivantes, nous explorerons comment implémenter LoRA en pratique et verrons ses avantages de première main.

## Installation

-   Assurez-vous d'être sur une instance GPU
-   Installez les packages requis pour le fine-tuning ; voir le Hub de Finetuning LLM
-   Installez PEFT depuis la source pour les nouvelles fonctionnalités
-   Redémarrez l'instance si nécessaire
